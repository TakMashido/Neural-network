add other n,m in gng network for neurons connected to winner
add option to stop learning on set acumulated error value
add dynamic n and m for each layer(baseN/connectionNumber, baseM/connectionNumber)
optymalize openCL and multithreading learning options. Slighty slower then one thread learning
check if making kerel for each layer can speed up(one time set up parameters)
add validational set
add ability to choose openCL device we want to run network at(for now device 0 in platform 0).
add removing low weights and low weights-neurons to optymalize network structure (good start to evolution-backpropagation algoritm)
improve wiki

Big TODO's for rewriting liblaries
change multithreading to perform multiple learnings in parannel to remove synchronization.
marge Teacher and LNetwork for better overwriting of networks learning parameters
Change LearningSequence to one object not array of it
